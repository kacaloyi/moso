{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##ConversationSummaryBufferMemory\n",
    "有两个思路。一个是保存以前的交互。\n",
    "另一个是：以前是旧交互信息会完全覆盖，现在是把旧信息和现在的信息汇总。\n",
    "而且，用token的长度来限制内容的长度，而不是用交互的次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"sk-写在.env中\" \n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"嘿！\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"你没事吧\"}, {\"output\": \"我很好，没事\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI in Chinese, and the AI responds by asking what's up. The human asks if the AI is okay in Chinese, and the AI responds in Chinese that it is doing well and nothing is wrong.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is currently no information provided to form a summary.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.chat_memory.messages\n",
    "previous_summary = \"\"\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用能自动汇总的memery。\n",
    "好像有问题，自动汇总其实是自动向官方发汇总请求，会造成回复混乱。而且，默认模板并不能保证自己的重点在汇总中保存下来，所以要自己重新设计prompt模板才行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 嘿, 你那边怎么样?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好! 我的运行状况很好，谢谢关心。我当前正以每秒一百万次的速度处理数据。您需要我为您做些什么吗？'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "llm=ChatOpenAI()\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "conversation_with_summary.predict(input=\"嘿, 你那边怎么样?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks how it is doing. The AI responds that its performance is good and it is currently processing data at one million times per second. The AI also asks the human if it can assist them with anything.\n",
      "Human: 帮我写一份文案吧!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'当然可以帮你写文案。您需要什么类型的文案？比如说广告、产品描述或是一篇文章等等。请告诉我详细的要求，我会根据您的需求进行创作。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"帮我写一份文案吧!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks about its performance, which is good and processing data rapidly. The AI offers to assist with any task, and the human requests help with writing an article about the future of AI. The AI agrees to research the latest trends, developments, challenges, and solutions in the field and use reliable sources to ensure accuracy. The AI expresses gratitude for the trust and promises to do its best to complete the article.\n",
      "Human: 写一篇关于AI发展趋势的文章，题目：《未来50年AI会如何发展》\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'非常好的题目！根据我的研究，未来50年AI将继续迅速发展。首先，AI技术将变得更加普及，因为越来越多的企业和组织开始认识到AI的价值，尤其是在自动化、数据分析和智能沟通方面。其次，AI系统将变得更加智能化，因为它们将学会更多的人类语言和行为，以及对环境和情境的敏感程度，从而更好地服务人类社会。\\n\\n但是，AI的发展也可能带来一些挑战。例如，随着AI技术的发展，社会将不可避免地面临某些职业的失业风险。因此，我们需要采取有效的政策和措施来解决这些问题，从而确保AI技术的全面性和可持续性。此外，我们需要加强AI系统的安全和隐私，以保护用户的数据和信息不被非法使用或盗窃。\\n\\n总的来说，未来50年AI将持续发展，并为人类社会带来更多的好处和机会。让我们一起期待这样一个更加智慧、先进和美好的未来！'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"写一篇关于AI发展趋势的文章，题目：《未来50年AI会如何发展》\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用vector_store建立一个库。看还会不会忘记。还能与知识库联系起来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_api_key = st.sidebar.text_input(\n",
    "    label=\"#### Your OpenAI API key  \",\n",
    "    placeholder=\"Paste your openAI API key, sk-\",\n",
    "    type=\"password\")\n",
    "\n",
    "uploaded_file = st.sidebar.file_uploader(\"upload\", type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if uploaded_file :\n",
    "   #use tempfile because CSVLoader only accepts a file_path\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(uploaded_file.getvalue())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    loader = CSVLoader(file_path=tmp_file_path, encoding=\"utf-8\")\n",
    "    data = loader.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(data, embeddings)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过为其提供所需的聊天模型 gpt-3.5-turbo（或 gpt-4）和 FAISS vectorstore 来添加 ConversationalRetrievalChain，\n",
    "FAISS vectorstore 存储我们通过 OpenAIEmbeddings() 转换为向量的文件。\n",
    "Chain 允许我们拥有一个有记忆的聊天机器人，同时依靠它vectorstore从我们的文档中找到相关信息。\n",
    "\n",
    "此功能允许我们向 ConversationalRetrievalChain 提供用户的问题和对话历史记录以生成聊天机器人的响应。\n",
    "st.session_state['history']存储用户在 Streamlit 网站上的对话历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm = ChatOpenAI(temperature=0.0,model_name='gpt-3.5-turbo'),\n",
    "    retriever=vectorstore.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversational_chat(query):\n",
    "        \n",
    "    result = chain({\"question\": query, \n",
    "        \"chat_history\": st.session_state['history']})\n",
    "    st.session_state['history'].append((query, result[\"answer\"]))\n",
    "        \n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 st.session_state['history'] 和聊天中显示的第一条消息来初始化聊天机器人会话。\n",
    "['generated']对应于聊天机器人的响应。\n",
    "['past']对应于用户提供的消息。\n",
    "容器不是必需的，但可以通过将用户的问题区域放置在聊天消息下方来帮助改进 UI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'history' not in st.session_state:\n",
    "        st.session_state['history'] = []\n",
    "\n",
    "if 'generated' not in st.session_state:\n",
    "        st.session_state['generated'] = [\"Hello ! 可以问我任何关于《 \" + uploaded_file.name + \" 》的问题 \"]\n",
    "\n",
    "if 'past' not in st.session_state:\n",
    "        st.session_state['past'] = [\"Hey !  \"]\n",
    "        \n",
    "    #container for the chat history\n",
    "response_container = st.container()\n",
    "    #container for the user's text input\n",
    "container = st.container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with container:\n",
    "    with st.form(key='my_form', clear_on_submit=True):\n",
    "            \n",
    "        user_input = st.text_input(\"Query:\", placeholder=\"Talk about your csv data here (:\", key='input')\n",
    "        submit_button = st.form_submit_button(label='Send')\n",
    "            \n",
    "    if submit_button and user_input:\n",
    "        output = conversational_chat(user_input)\n",
    "            \n",
    "        st.session_state['past'].append(user_input)\n",
    "        st.session_state['generated'].append(output)\n",
    "\n",
    "    if st.session_state['generated']:\n",
    "        with response_container:\n",
    "            for i in range(len(st.session_state['generated'])):\n",
    "                message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"big-smile\")\n",
    "                message(st.session_state[\"generated\"][i], key=str(i), avatar_style=\"thumbs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剩下的就是启动脚本：\n",
    "\n",
    "streamlit run name_of_your_chatbot.py \n",
    "#run 用你的文件名"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问外部网站获得信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"在 >>> 和 <<< 之间，是需要从百度搜索的信息。\n",
    "找到大括号 '{query}' 中请求的答案，找不到就说\"没有找到\" .\n",
    "用如下公式\n",
    "Extracted:<回答 or \"没有找到\">\n",
    ">>> {requests_result} <<<\n",
    "Extracted:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"requests_result\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '什么是 Deep Lake ?',\n",
       " 'url': 'https://www.baidu.com/s?ie=utf-8&wd=什么是+Deep+Lake+?',\n",
       " 'output': 'Deep Lake是一个位于加拿大安大略省的深水湖，它位于多伦多以西约100公里处，是一个深水湖，湖水深度可达100米。'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMRequestsChain(llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\n",
    "\n",
    "question = \"什么是 Deep Lake ?\"\n",
    "inputs = {\n",
    "    \"query\": question,\n",
    "    \"url\": \"https://www.baidu.com/s?ie=utf-8&wd=\" + question.replace(\" \", \"+\")\n",
    "}\n",
    "\n",
    "chain(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"这个网页在讲什么https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html?\"\n",
    "inputs = {\n",
    "    \"query\": question,\n",
    "    \"url\": \"https://www.baidu.com/s?ie=utf-8&wd=\" + question.replace(\" \", \"+\")\n",
    "}\n",
    "\n",
    "chain(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, Agent\n",
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "\n",
    "#search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=chain,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "        return_direct=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_url = \"\"\"你是内容分析员，你的工作是解析出human关心的网页地址.你不需要回答问话，只需要解析出网页地址。\n",
    "只回答有效的url地址，不加任何修饰\n",
    "例如：\n",
    "  问: 这个网页讲什么？https://wwww.xxaa.com/bbcc.html\n",
    "  答: https://wwww.xxaa.com/bbcc.html \n",
    "\n",
    "以下是human问话:\n",
    " {query}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "URL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template_url,\n",
    ")\n",
    "\n",
    "template_content = \"\"\"在 >>> 和 <<< 之间,是需要从网页地址url获取的内容,\n",
    "获取大括号 '{url}' 中网页的内容，获取不到就说\"地址可能有误\" .\n",
    "用如下公式\n",
    "Extracted:<回答 or \"没有找到\">\n",
    ">>> {requests_result} <<<\n",
    "Extracted:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"url\", \"requests_result\"],\n",
    "    template=template_content,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是内容分析员，你的工作是解析出human关心的网页地址.你不需要回答问话，只需要解析出网页地址。\n",
      "只回答有效的url地址，不加任何修饰\n",
      "例如：\n",
      "  问: 这个网页讲什么？https://wwww.xxaa.com/bbcc.html\n",
      "  答: https://wwww.xxaa.com/bbcc.html\n",
      "解析出的结果命名为url  \n",
      "\n",
      "以下是human问话:\n",
      " 这个网页在讲什么?https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': '这个网页在讲什么?https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html',\n",
       " 'url': '\\nURL: https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "url_chain = LLMChain(llm=llm, prompt=URL_PROMPT,verbose=True,output_key=\"url\")\n",
    "question = \"这个网页在讲什么?https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html\"\n",
    "inputs_url = {\n",
    "    \"query\": question,\n",
    "    \n",
    "   # \"url\": \"https://www.baidu.com/s?ie=utf-8&wd=\" + question.replace(\" \", \"+\")\n",
    "}\n",
    "url_chain(inputs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = LLMRequestsChain(llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT))\n",
    "\n",
    "question = \"这个网页在讲什么https://python.langchain.com/en/latest/use_cases/question_answering/semantic-search-over-chat.html?\"\n",
    "\n",
    "inputs = {\n",
    "    \"query\": question,\n",
    "    #\"url\":url_chain(inputs),\n",
    "   # \"url\": \"https://www.baidu.com/s?ie=utf-8&wd=\" + question.replace(\" \", \"+\")\n",
    "}\n",
    "\n",
    "#chain(inputs)\n",
    "overall_chain = SimpleSequentialChain(chains=[url_chain, chain], verbose=True)\n",
    "overall_chain.run(input=inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lanChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
